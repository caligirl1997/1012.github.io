<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
  <title>choices</title>
  	<link rel="stylesheet" href="../styles/mystyles.css">
  </head>

<body>
<img src="https://images.theconversation.com/files/279485/original/file-20190614-158921-1kbfer5.jpg?ixlib=rb-1.1.0&q=45&auto=format&w=496&fit=clip" alt="Enhancing Social Good in Computing"
<!-- Site navigation menu -->

<ul class="navbar">
  <li><a href="../index.html">Home page</a>
  <li><a href="topic.html">Technology/Topic</a>
	<li><a href="opportunities.html">Opportunities</a>
	<li><a href="risks.html">Risks</a>
	<li><a href="choices.html">Choices</a>
	<li><a href="ethics.html">Ethical Reflections</a>
	<li><a href="references.html">References</a>
   <li><a href="process.html">Process Support</a>
</ul>

<!-- Main content -->
<h1>A.I. Choices</h1>


</ul><h2><p>Who is responsible for the decisions A.I. make?<p></h2>
</ul><h2><p>How much trust do we put In to A.I. technology to make ethical decisions?<p></h2>	
</ul><h2><p>How do we regulate its decision-making capabilities?<p></h2>	
</ul><h2><p>Can we trust society with not taking advantage of the technologyâ€™s capabilities for unethical reasons?<p></h2>

</ul><h3><p>People are likely to instantly take credit for any positive advancements or findings, but what about when things do not go to plan? There will be many factors to consider when A.I. begins to act up. It would seem easy to blame engineers and programmers for creating a flawed system, but it is far more complex than this. For example, a technology having a single stakeholder will not always be the case. A.I. may be created using multiple stakeholders in order improve the overall efficiency and quality by assigning more specialised tasks to better suited people. This begins to complicate things, as individuals or groups may be become scapegoats for criticism even if the faults were not theirs. A.I. will need to find a way to be governed correctly and fairly. Accountability will need to be definitive to find who is at fault. A.I. governance will need to establish concrete regulations and ethical frameworks to avoid any ethical conflicts which may arise (Cath, 2018). With rules and regulations in place, we then need to establish a level of trust with the technology itself and society. The trust we give A.I. to make ethical decisions is up to the regulators and creators. They need to decide when to draw the line and not give an overreliance to the technology. Overreliance could make it much more difficult to hold someone accountable. We also need to keep in mind if we can trust people to not take advantage of A.I. for unethical reasons such as stealing. Crime rates are never going to be absolute zero, so trusting all members of society to not take advantage of the technology will be extremely difficult. Steps may be taken in order to build trust over time and prevent any wrongdoings which people may use A.I. to commit.<p></h3>

<!-- Sign and date the page, it's only polite! -->
<address>Made 1 March 2021<br>
  by Tony Clear.</address>
 
<p><em>thanks to W3C for tutorial and adapted code from <a href="https://www.w3.org/Style/Examples/011/firstcss.en.html">Style Examples</a></p></em>
<p><em>also thanks to WDN for HTML and CSS resources and any adapted code snippets from <a href= "https://developer.mozilla.org/en-US/docs/Web">Mozilla Developer Network</a></p></em 
 </html>
